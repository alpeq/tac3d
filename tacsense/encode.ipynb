{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch as snn\n",
    "import snntorch.spikeplot as splt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from snntorch import spikegen, surrogate, utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from IPython.display import HTML\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net_default = {\n",
    "    'spike_grad': surrogate.fast_sigmoid(slope=25),\n",
    "    'beta': 0.5,\n",
    "    'num_steps': 50\n",
    "}\n",
    "\n",
    "\n",
    "class TouchDataset(Dataset):\n",
    "    def __init__(self, filename, transform=None) -> None:\n",
    "        \"\"\"Constructor for TouchDataset.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): File Name of the touch data.\n",
    "            transform (Compose, optional): Compose of transforms to apply on samples. Defaults to None.\n",
    "        \"\"\"\n",
    "        data = np.load(filename, allow_pickle=True)\n",
    "        sensordata = [np.array(data[k]['sensordata']) for k in data.keys()]\n",
    "        orientation = [np.array(data[k]['orientations'])for k in data.keys()]\n",
    "        self.sensordata, self.orientation = np.vstack(\n",
    "            sensordata), np.hstack(orientation)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.orientation)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.sensordata[index, :, :]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, self.orientation[index]\n",
    "\n",
    "\n",
    "class MinMaxScale(object):\n",
    "    def __init__(self, minVal, maxVal):\n",
    "        \"\"\"Initialize the transform to rescale the values into [minVal, maxVal].\n",
    "\n",
    "        Args:\n",
    "            minVal (float): Minimum value.\n",
    "            maxVal (float): Maximum value.\n",
    "        \"\"\"\n",
    "        self.min, self.max = minVal, maxVal\n",
    "\n",
    "    def __call__(self, x):\n",
    "        min_x, max_x = np.min(x), np.max(x)\n",
    "        if min_x >= max_x:\n",
    "            y = 0.0\n",
    "        else:\n",
    "            y = (x - min_x)/(max_x - min_x)*(self.max - self.min) + self.min\n",
    "        return y\n",
    "\n",
    "\n",
    "class ToSpike(object):\n",
    "    def __init__(self, encoding='rate', **kwargs):\n",
    "        \"\"\"Initialize the spike generator.\n",
    "\n",
    "        Args:\n",
    "            encoding (str, optional): Spiking encoding options. Defaults to 'rate'.\n",
    "        \"\"\"\n",
    "        self._encodings = ['rate', 'latency', 'delta']\n",
    "        if encoding in self._encodings:\n",
    "            self.encoding = encoding\n",
    "        else:\n",
    "            self.encoding = self._encodings[0]\n",
    "        self.configs = kwargs\n",
    "        match self.encoding:\n",
    "            case 'rate':\n",
    "                default = {\n",
    "                    'num_steps': 100,\n",
    "                    'gain': 0.5\n",
    "                }\n",
    "            case 'latency':\n",
    "                deafult = {\n",
    "\n",
    "                }\n",
    "            case 'delta':\n",
    "                default = {\n",
    "\n",
    "                }\n",
    "            case _:\n",
    "                default = {\n",
    "\n",
    "                }\n",
    "        self.configs = default | self.configs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        match self.encoding:\n",
    "            case 'rate':\n",
    "                y = spikegen.rate(\n",
    "                    x, num_steps=self.configs['num_steps'], gain=self.configs['gain'])\n",
    "            case _:\n",
    "                y = x\n",
    "        return y\n",
    "\n",
    "\n",
    "class TacNet(nn.Module):\n",
    "    def __init__(self, dim_input, dim_output, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_output = dim_output\n",
    "        self.configs = kwargs | net_default\n",
    "\n",
    "        # Initialize Network\n",
    "        # autopep8: off\n",
    "        self.net = nn.Sequential(nn.Conv2d(1, 12, 5),\n",
    "                                 nn.MaxPool2d(2),\n",
    "                                 snn.Leaky(beta=self.configs['beta'], spike_grad=self.configs['spike_grad'], init_hidden=True),\n",
    "                                 nn.Conv2d(12, 64, 5),\n",
    "                                 nn.MaxPool2d(2),\n",
    "                                 snn.Leaky(beta=self.configs['beta'], spike_grad=self.configs['spike_grad'], init_hidden=True),\n",
    "                                 nn.Flatten(),\n",
    "                                 snn.Leaky(beta=self.configs['beta'], spike_grad=self.configs['spike_grad'], init_hidden=True, output=True))\n",
    "        # autopep8: on\n",
    "\n",
    "    def forward(self, num_steps, x):\n",
    "        mem_rec = []\n",
    "        spk_rec = []\n",
    "        # resets hidden states for all LIF neurons in net\n",
    "        utils.reset(self.net)\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            spk_out, mem_out = self.net(x)\n",
    "            spk_rec.append(spk_out)\n",
    "            mem_rec.append(mem_out)\n",
    "\n",
    "        return torch.stack(spk_rec), torch.stack(mem_rec)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Touch Dataset with Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 wngfra.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "batch_size = 128\n",
    "num_steps = 1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    MinMaxScale(0., 1.0),\n",
    "    transforms.ToTensor(),\n",
    "    ToSpike('rate', num_steps=num_steps)\n",
    "])\n",
    "touch_dataset = TouchDataset('../data/touch.pkl', transform=transform)\n",
    "train_loader = DataLoader(touch_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = TacNet(1, 10).to(device)\n",
    "# Iterate through minibatches\n",
    "data = iter(train_loader)\n",
    "spike_data, _ = next(data)\n",
    "x = spike_data.float().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. F. Pascal, L. Bombrun, J. -Y. Tourneret and Y. Berthoumieu, \"Parameter Estimation For Multivariate Generalized Gaussian Distributions,\" in IEEE Transactions on Signal Processing, vol. 61, no. 23, pp. 5960-5971, Dec.1, 2013, doi: 10.1109/TSP.2013.2282909.\n",
    "2. A. Parvizi-Fard, M. Amiri, D. Kumar, M. M. Iskarous, and N. V. Thakor, “A functional spiking neuronal network for tactile sensing pathway to process edge orientation,” Sci Rep, vol. 11, no. 1, p. 1320, Dec. 2021, doi: 10.1038/s41598-020-80132-4.\n",
    "3. J. A. Pruszynski and R. S. Johansson, “Edge-orientation processing in first-order tactile neurons,” Nat Neurosci, vol. 17, no. 10, pp. 1404–1409, Oct. 2014, doi: 10.1038/nn.3804.\n",
    "4. J. M. Yau, S. S. Kim, P. H. Thakur, and S. J. Bensmaia, “Feeling form: the neural basis of haptic shape perception,” Journal of Neurophysiology, vol. 115, no. 2, pp. 631–642, Feb. 2016, doi: 10.1152/jn.00598.2015.\n",
    "5. G. Sutanto, Z. Su, S. Schaal, and F. Meier, “Learning Sensor Feedback Models from Demonstrations via Phase-Modulated Neural Networks,” in 2018 IEEE International Conference on Robotics and Automation (ICRA), Brisbane, QLD, May 2018, pp. 1142–1149. doi: 10.1109/ICRA.2018.8460986."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tac3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36265a5cec26929347de1c5b3c57cab671b09d9256f1cdd55044ea70b0305c31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
