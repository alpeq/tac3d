{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch as snn\n",
    "import snntorch.spikeplot as splt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from snntorch import spikegen, surrogate, utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from IPython.display import HTML\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net_default = {\n",
    "    'spike_grad': surrogate.fast_sigmoid(slope=25),\n",
    "    'beta': 0.5,\n",
    "    'num_steps': 50\n",
    "}\n",
    "\n",
    "\n",
    "class TouchDataset(Dataset):\n",
    "    def __init__(self, filename, transform=None) -> None:\n",
    "        \"\"\"Constructor for TouchDataset.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): File Name of the touch data.\n",
    "            transform (Compose, optional): Compose of transforms to apply on samples. Defaults to None.\n",
    "        \"\"\"\n",
    "        data = np.load(filename, allow_pickle=True)\n",
    "        sensordata = [np.array(data[k]['sensordata']) for k in data.keys()]\n",
    "        orientation = [np.array(data[k]['orientations'])for k in data.keys()]\n",
    "        self.sensordata, self.orientation = np.vstack(\n",
    "            sensordata), np.hstack(orientation)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.orientation)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.sensordata[index, :, :]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, self.orientation[index]\n",
    "\n",
    "\n",
    "class MinMaxScale(object):\n",
    "    def __init__(self, minVal, maxVal):\n",
    "        \"\"\"Initialize the transform to rescale the values into [minVal, maxVal].\n",
    "\n",
    "        Args:\n",
    "            minVal (float): Minimum value.\n",
    "            maxVal (float): Maximum value.\n",
    "        \"\"\"\n",
    "        self.min, self.max = minVal, maxVal\n",
    "\n",
    "    def __call__(self, x):\n",
    "        min_x, max_x = np.min(x), np.max(x)\n",
    "        if min_x >= max_x:\n",
    "            y = 0.0\n",
    "        else:\n",
    "            y = (x - min_x)/(max_x - min_x)*(self.max - self.min) + self.min\n",
    "        return y\n",
    "\n",
    "\n",
    "class ToSpike(object):\n",
    "    def __init__(self, encoding='rate', **kwargs):\n",
    "        \"\"\"Initialize the spike generator.\n",
    "\n",
    "        Args:\n",
    "            encoding (str, optional): Spiking encoding options. Defaults to 'rate'.\n",
    "        \"\"\"\n",
    "        self._encodings = ['rate', 'latency', 'delta']\n",
    "        if encoding in self._encodings:\n",
    "            self.encoding = encoding\n",
    "        else:\n",
    "            self.encoding = self._encodings[0]\n",
    "        self.configs = kwargs\n",
    "        match self.encoding:\n",
    "            case 'rate':\n",
    "                default = {\n",
    "                    'num_steps': 100,\n",
    "                    'gain': 0.5\n",
    "                }\n",
    "            case 'latency':\n",
    "                deafult = {\n",
    "\n",
    "                }\n",
    "            case 'delta':\n",
    "                default = {\n",
    "\n",
    "                }\n",
    "            case _:\n",
    "                default = {\n",
    "\n",
    "                }\n",
    "        self.configs = default | self.configs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        match self.encoding:\n",
    "            case 'rate':\n",
    "                y = spikegen.rate(\n",
    "                    x, num_steps=self.configs['num_steps'], gain=self.configs['gain'])\n",
    "            case _:\n",
    "                y = x\n",
    "        return y\n",
    "\n",
    "\n",
    "class TacNet(nn.Module):\n",
    "    def __init__(self, dim_input, dim_output, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_output = dim_output\n",
    "        self.configs = kwargs | net_default\n",
    "\n",
    "        #  Initialize Network\n",
    "        # FIXME conv size mismatch\n",
    "        # autopep8: off\n",
    "        self.net = nn.Sequential(nn.Conv2d(1, 12, 5),\n",
    "                                 nn.MaxPool2d(2),\n",
    "                                 snn.Leaky(beta=self.configs['beta'], spike_grad=self.configs['spike_grad'], init_hidden=True),\n",
    "                                 nn.Conv2d(12, 64, 5),\n",
    "                                 nn.MaxPool2d(2),\n",
    "                                 snn.Leaky(beta=self.configs['beta'], spike_grad=self.configs['spike_grad'], init_hidden=True),\n",
    "                                 nn.Flatten(),\n",
    "                                 nn.Linear(64*4*4, self.dim_output),\n",
    "                                 snn.Leaky(beta=self.configs['beta'], spike_grad=self.configs['spike_grad'], init_hidden=True, output=True)\n",
    "                                ).to(device)\n",
    "        # autopep8: on\n",
    "\n",
    "    def forward(self, num_steps, x):\n",
    "        mem_rec = []\n",
    "        spk_rec = []\n",
    "        # resets hidden states for all LIF neurons in net\n",
    "        utils.reset(self.net)\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            spk_out, mem_out = self.net(x)\n",
    "            spk_rec.append(spk_out)\n",
    "            mem_rec.append(mem_out)\n",
    "\n",
    "        return torch.stack(spk_rec), torch.stack(mem_rec)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Touch Dataset with Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x1537 and 1024x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [88], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m spike_data, _ \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data)\n\u001b[1;32m     17\u001b[0m x \u001b[39m=\u001b[39m spike_data\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 18\u001b[0m model(num_steps, x\u001b[39m.\u001b[39;49mview(\u001b[39m1\u001b[39;49m, batch_size, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m~/miniconda3/envs/tac3d/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [87], line 138\u001b[0m, in \u001b[0;36mTacNet.forward\u001b[0;34m(self, num_steps, x)\u001b[0m\n\u001b[1;32m    135\u001b[0m utils\u001b[39m.\u001b[39mreset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet)\n\u001b[1;32m    137\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_steps):\n\u001b[0;32m--> 138\u001b[0m     spk_out, mem_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x)\n\u001b[1;32m    139\u001b[0m     spk_rec\u001b[39m.\u001b[39mappend(spk_out)\n\u001b[1;32m    140\u001b[0m     mem_rec\u001b[39m.\u001b[39mappend(mem_out)\n",
      "File \u001b[0;32m~/miniconda3/envs/tac3d/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tac3d/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tac3d/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tac3d/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x1537 and 1024x10)"
     ]
    }
   ],
   "source": [
    "# Copyright 2022 wngfra.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "batch_size = 128\n",
    "num_steps = 100\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    MinMaxScale(0., 1.0),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "touch_dataset = TouchDataset('../data/touch.pkl', transform=transform)\n",
    "train_loader = DataLoader(touch_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = TacNet(1, 10)\n",
    "# Iterate through minibatches\n",
    "data = iter(train_loader)\n",
    "spike_data, _ = next(data)\n",
    "x = spike_data.float().to(device)\n",
    "model(num_steps, x.view(1, batch_size, -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. F. Pascal, L. Bombrun, J. -Y. Tourneret and Y. Berthoumieu, \"Parameter Estimation For Multivariate Generalized Gaussian Distributions,\" in IEEE Transactions on Signal Processing, vol. 61, no. 23, pp. 5960-5971, Dec.1, 2013, doi: 10.1109/TSP.2013.2282909.\n",
    "2. A. Parvizi-Fard, M. Amiri, D. Kumar, M. M. Iskarous, and N. V. Thakor, “A functional spiking neuronal network for tactile sensing pathway to process edge orientation,” Sci Rep, vol. 11, no. 1, p. 1320, Dec. 2021, doi: 10.1038/s41598-020-80132-4.\n",
    "3. J. A. Pruszynski and R. S. Johansson, “Edge-orientation processing in first-order tactile neurons,” Nat Neurosci, vol. 17, no. 10, pp. 1404–1409, Oct. 2014, doi: 10.1038/nn.3804.\n",
    "4. J. M. Yau, S. S. Kim, P. H. Thakur, and S. J. Bensmaia, “Feeling form: the neural basis of haptic shape perception,” Journal of Neurophysiology, vol. 115, no. 2, pp. 631–642, Feb. 2016, doi: 10.1152/jn.00598.2015.\n",
    "5. G. Sutanto, Z. Su, S. Schaal, and F. Meier, “Learning Sensor Feedback Models from Demonstrations via Phase-Modulated Neural Networks,” in 2018 IEEE International Conference on Robotics and Automation (ICRA), Brisbane, QLD, May 2018, pp. 1142–1149. doi: 10.1109/ICRA.2018.8460986."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tac3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b81b3919ee5f2218cc5f3be42eecb9a1fdcc9dc6e8f946db5ad2423192a42a92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
